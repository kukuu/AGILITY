
# Digital Transformation 

Is the change associated with the application of digital technology. When planning for digital transformation, organizations must factor the cultural changes they'll confront as workers and organizational leaders adjust to adopting and relying on unfamiliar technologies. Practices used in transformation include methodologies like Agile (SCRUM, KANBAN) , Continuous Delivery, Micro-services, Containerization, Big Data Analytics, Machine Learning, Change Management, Cloud Computing, Digital Security, Leadership & Development,IoT, and Mobility Management.


## Agile management

Agile management involves the division of a company into small teams of employees, each one acting like a start-up in its own right. The aim is to change the way in which your company runs to meet the ever-changing demands of the global market. The ‘Agile Manifesto,’ highlights key messages behind the system – including self-organisation, collaboration and cross-functionality of teams.

### Squads

The individual teams that make up a company in agile management are known as squads. The idea is that each squad has its own defined goal, which they work towards autonomously. Each squad has a ‘product owner’: they prioritise work to be done. However, they don’t tell employees how to work. Squad members also have access to an ‘agile coach’ to keep them up to date and well informed. Example: The 'Shopping  UI Basket', 'Platform Gateway', and 'Platform Core' teams.

### Tribes

A tribe is the name given to a collection of squads working together. They can be seen as an incubator for the mini start-up squads in your company. Tribes will usually not exceed more than around 100 employees. Example: Shopping 'Backend' and 'Test' Teams

### Guilds

The idea behind guilds is that they glue the company together and ensure you avoid losing economies of scale without sacrificing too much autonomy. In other words, guilds allow for cross-referencing between squads. Example: 'Shopping'


## Benefits of AGILE

#### 1. Revenue

The iterative nature of agile development means features are delivered incrementally, enabling some benefits to be realised early as the product continues to develop.


#### 2. Speed-to-market

Research suggests about 80% of all market leaders were first to market. As well as the higher revenue from incremental delivery, agile development philosophy also supports the notion of early and regular releases, and ‘perpetual beta’.


#### 3. Quality

A key principle of agile development is that testing is integrated throughout the cycle, enabling regular inspection of the working product as it develops. This allows the product owner to make adjustments if necessary and gives the product team early sight of any quality issues.


#### 4. Visibility

Agile development principles encourage active user involvement throughout the product’s development and a very cooperative collaborative approach. This provides excellent visibility  for key stakeholders, both of the project’s progress and of the product itself, which in turn helps to ensure that expectations are effectively managed.


#### 5. Risk Management

Small incremental releases made visible to the product owner and product team through its development help to identify any issues early and make it easier to respond to change. The clear visibility in agile development helps to ensure that any necessary decisions can be taken at the earliest possible opportunity, while there’s still time to make a material difference to the outcome.


#### 6. Flexibility / Agility

In traditional development projects, we write a big spec up-front and then tell business owners how expensive it is to change anything, particularly as the project goes on. In fear of scope creep and a never-ending project, we resist changes and put people through a change control committee to keep them to the essential minimum. Agile development principles are different. In agile development, change is accepted. In fact, it’s expected. Because the one thing that’s certain in life is change. Instead the timescale is fixed and requirements emerge and evolve as the product is developed. Of course for this to work, it’s imperative to have an actively involved  stakeholder who understands this concept and makes the necessary trade-off decisions, trading existing scope for new.


#### 7. Cost Control

The above approach of fixed timescales and evolving requirements enables a fixed budget. The scope of the product and its features are variable, rather than the cost.


#### 8. Business Engagement/Customer Satisfaction
 
The active involvement of a user representative and/or product owner, the high visibility  of the product and progress, and the flexibility to change when change is needed, create much better business engagement and customer satisfaction. This is an important benefit that can create much more positive and enduring working relationships.


#### 9. Right Product

Above all other points, the ability for agile development requirements to emerge and evolve, and the ability to embrace change (with the appropriate trade-offs), the team build the right product. It’s all too common in more traditional projects to deliver a “successful” project in IT terms and find that the product is not what was expected, needed or hoped for. In agile development, the emphasis is absolutely on building the right product.


#### 10. More Enjoyable!

The active involvement, cooperation and collaboration make agile development teams a much more enjoyable place for most people. Instead of big specs, we discuss requirements in workshops. Instead of lengthy status reports, we collaborate around a task-board discussing progress. Instead of long project plans and change management committees, we discuss what’s right for the product and project and the team is empowered to make decisions. In my experience this makes it a much more rewarding approach for everyone. In turn this helps to create highly motivated, high performance teams that are highly co-operative.



## Implementing SAFe Lean Agile Development

SAFe emphasizes the values of autonomous, self-organisng, cross-functional teams and Agile Release Trains.
This supports a leaner management infrastructure, with more empowered individuals and teams and faster, 
local decision making. Taditional, day-to-day employee instructions and activity  direction is no longer required.

However, all employees  still need someone to assist them with career development, set and manage expectations
and provide the active coaching they need to advance their technical, functional, individual and team 
skills and career goals.  They also have a right to serve as an integral member of a high performing team.

Self organising ARTs do not fund themselves or define their own mission. That remains a management responsibility,
as it is an element of implementation of a  strategy.

Much of this responsibility falls to the traditional role of the Dvelopment or Engineering Manager.

The Development or Engineering Manager exhibits the principles and practices of Lean-Agile leadership. 
Coaching Scrum teams to deliver effectively, at a sustainable pace, and as Advisor
to use agile engineering practices such as automated builds and TDD. 

Further, the Manager  takes responsibility for eliminating impediments, actively evolves the systems
in which all knowledge workers operate, takes subtle control in assigning individuals to teams, 
participates in defining and administering compensation, benefilts and promotions, 
evaluate performance, including team input and corrective actions, remains close 
enough to the team to add value and to be competent, input into training and development strategy,
organising training and themed dev community months/internal developer conferences.

Other duties will include involved in recruitment and retention strategy, on-boarding new developers, 
co-facilitating development community of practice, appraisals & Performance Management, 
organising recognition awards and socials, and finally stays far away to let them problem-solve on their own.


## How are Scrum and Kanban different

There are a number of differences in both the philosophy behind and the practical application of Scrum and Kanban. While the individual differences are many, they can be grouped into the following three buckets:

### Scheduling, iteration, and cadence

1. Scrum processes place heavy emphasis on schedule. The scrum team is provided with a prioritized list of story points that need to be completed to deliver a shippable product. The team must decide how many of the points they feel can be completed within one sprint. Anything outside the scope they commit to must wait for the next sprint. Optimally, an efficient scrum team will quickly learn their capabilities over the course of several sprints and their estimates will improve and be optimized as time goes on. Then, every two weeks (or however long their sprint is) the team produces a shippable product, carries out a retrospective to discuss optimizing the process, and moves into the next sprint. This iterative process is designed to allow for accurate estimations of work flow and effective management of multiple projects. 

2. On a Kanban team, there are no required time boxes or iterations. While the Kanban method is iterative in nature, the continual improvement is expected to occur in an evolutionary fashion as work is continually completed. The limitations placed on various conditions in the work flow will be regulated early in a team’s (or organization’s) use of Kanban until an optimal set of limits is arrived at to keep the flow steady and efficient.


### Roles and responsibilities

1. On scrum teams, there are at least three roles that must be assigned in order to effectively process the work: the Product Owner, Scrum Master, and Team Members. Each role has its own set of responsibilities, and they must work together to achieve an orderly and efficient balance. The scrum team itself also must be cross-functional, which is to say that one team must have all the resources necessary to complete the entire sprint’s work.

2. Under Kanban, no set roles are prescribed. Practically speaking, it makes sense for someone to serve as a project manager or supervisor, especially for larger more complex Kanban projects, but the roles should theoretically evolve with the needs of the project and the organization. 

3. A Kanban team is not required to be cross-functional since the Kanban work flow is intended to be used by any and all teams involved in the project. Therefore, a team of specialists and a separate team of generalists may be working on different aspects of the same Kanban project from the same board, and that’s ok.

# Continuous Delivery (RRASc)

#### Reliable :: Repeatable :: Automation :: Source control

 CD is the ability to get changes of all types including new features, configuration changes, bug fixes and experiments into production, or into the hands of users, safely and quickly in a sustainable way. 

 A logical extension of Continuous Integration, It is based on the use of smart automation. This is all about creating a repeatable and reliable process for delivering software. You have to automate pretty much everything in order to be able to achieve continuous delivery. Manual steps will get in the way or become a bottleneck. This goes for everything from requirements authoring to deploying to production.



The ultimate goal of continuous delivery is to minimise the iteration time of the code-test-deliver-measure experimentation cycle. Increasing deliverable throughput in this way is the key to not only more feature work being delivered but higher quality code as well. This might seem counter-intuitive at first but code is fixed and polished through that same cycle and less time spent on deployment is more time spent on designing quality code.

	The high-level requirements FOR CD are:

	1. Software must be easily testable, which means it must be loosely coupled.

	2. Delivery must—under normal circumstances—require minimal human interaction.

	3. Delivery—from commit to production—must be fast. Preferably under 10 minutes.

	4. Rolling back a deployed feature if it is found to be broken or unwanted must be trivial.

	5. Build binaries only once. Binaries should be compiled once and once only. 
	   The binary should then be stored someplace which is accessible only to your deployment mechanism, 
	   and your deployment mechanism should deploy this same binary to each successive environment

	6. Use precisely the same mechanism to deploy to every environment. Both QA and production
	   deployment must be both automated.

	7. Smoke test your deployment. Write a smoke test and include that in the deployment process.

	8. Stop the lines if anything fails.


	Achieving CD

	1. The process for releasing/deploying software MUST be repeatable and reliable. 

	2. Automate everything!

	3. If somethings difficult or painful, do it more often.

	4. Keep everything in source control

	5. Done means “released”.

	6. Build quality in! 

	7. Everybody has responsibility for the release process

	8. Improve continuously


The most optimum path to achieve these goals is to use  microservices architecture


# Micro-services 

Microservices architecture is an approach to application development in which a large application is built as a suite of modular services. Each module supports a specific business goal and uses a simple, well-defined interface to communicate with other sets of services.

When you choose to build your application as a set of microservices, you need to decide how your application’s clients will interact with the microservices. With a monolithic application there is just one set of (typically replicated, load‑balanced) endpoints. In a microservices architecture, however, each microservice exposes a set of what are typically fine‑grained endpoints.

Services must handle requests from the application’s clients. Furthermore, services must sometimes collaborate to handle those requests. They must use an inter-process communication protocol. Use asynchronous messaging for inter-service communication. Services communicating by exchanging messages over messaging channels. Examples of asynchronous messaging technologies are 
Apache Kafka and RabbitMQ.

## This pattern has the following benefits:

1. Loose coupling since it decouples client from services

2. Improved availability since the message broker buffers messages until the consumer is able to process them

3. Supports a variety of communication patterns including request/reply, notifications, request/async response, publish/subscribe, publish/async response etc


## This pattern has the following drawbacks:

1. Additional complexity of message broker, which must be highly available

2. This pattern has the following issues:

i. Request/reply-style communication is more complex

i. Client needs to discover location of message broker


#### Loose coupling::individual deployment:: repeatable deployment can only be achieved by repeatble context ==> containerisation

Microservices enforce loose coupling, plus it’s easier to develop fast and reliable deployment pipelines if they only have to handle small packages.

However, note microservices introduced a new problem: if adding a feature was often going to require adding a new, independently deployed and hosted service then that process had to be fast and not require any specialist knowledge. 

Demanding that every single developer in the company learn the intricacies of maintaining Puppet configuration for all their services would have been impractical and more than a little cruel.

Resolution:

	1.	Set up feature teams  that can possibly set up a new service in under four hours. What this means
		is Developing services should not   require knowledge of the infrastructure and changing 
		infrastructure should not require detailed knowledge of the services running on it. 
		If we need to change the hostname or port a service runs on it should require no changes
		to the service itself.

	2.	All project configuration—from build process to health monitoring—must be contained within the 
		project repository. Anything else introduces hidden dependencies for deployment that threaten
		to break the pipeline and require specialist knowledge to debug.

	3.	The above configuration should be declarative and not require adding dependencies to the project.

	4.   Use Containerisation

### Challenges and bottlenecks to Micro-services

#### Direct Client‑to‑Microservice Communication

In theory, a client could make requests to each of the microservices directly. Each microservice would have a public endpoint (https://serviceName.api.company.name). This URL would map to the microservice’s load balancer, which distributes requests across the available instances. To retrieve the product details, the mobile client would make requests to each of the services listed above.

Unfortunately, there are challenges and limitations with this option. 

1. One problem is the mismatch between the needs of the client and the fine‑grained APIs exposed by each of the microservices. The client in this example has to make posibly a number of separate requests. In more complex applications it might have to make many more.  While a client could make that many requests over a LAN, it would probably be too inefficient over the public Internet and would definitely be impractical over a mobile network. This approach also makes the client code much more complex.

2. Another problem with the client directly calling the microservices is that some might use protocols that are not web‑friendly. One service might use Thrift binary RPC while another service might use the AMQP messaging protocol. Neither protocol is particularly browser‑ or firewall‑friendly and is best used internally. An application should use protocols such as HTTP and WebSocket outside of the firewall.

3. Another drawback with this approach is that it makes it difficult to refactor the microservices. Over time we might want to change how the system is partitioned into services. For example, we might merge two services or split a service into two or more services. If, however, clients communicate directly with the services, then performing this kind of refactoring can be extremely difficult.
Because of these kinds of problems it rarely makes sense for clients to talk directly to microservices.


#### Resolving micro service complexity  with an API Gateway

Usually a much better approach is to use what is known as an API Gateway. An API Gateway is a server (Serving as an interface) that is the single entry point into the system. It is similar to the Facade pattern from object‑oriented design. The API Gateway encapsulates the internal system architecture and provides an API that is tailored to each client. It might have other responsibilities such as authentication, monitoring, load balancing, caching, request shaping and management, and static response handling.

The Facade design pattern is often used when a system is very complex or difficult to understand because the system has a large number of interdependent classes or its source code is unavailable. This pattern hides the complexities of the larger system and provides a simpler interface to the client. It typically involves a single wrapper class that contains a set of members required by client. These members access the system on behalf of the facade client and hide the implementation details.

1. The API Gateway is responsible for request routing, composition, and protocol translation. All requests from clients first go through the API Gateway. It then routes requests to the appropriate microservice. 

2. The API Gateway will often handle a request by invoking multiple microservices and aggregating the results. It can translate between web protocols such as HTTP and WebSocket and web‑unfriendly protocols that are used internally.

3. The API Gateway can also provide each client with a custom API. It typically exposes a coarse‑grained API for mobile clients. Consider, for example, the product details scenario. The API Gateway can provide an endpoint (/productdetails?productid=xxx) that enables a mobile client to retrieve all of the product details with a single request. The API Gateway handles the request by invoking the various services – product info, recommendations, reviews, etc. – and combining the results.

### Benefits and Drawbacks of an API Gateway

Using an API Gateway has both benefits and drawbacks. 

Benefit:  A major benefit of using an API Gateway is that it encapsulates the internal structure of the application. Rather than having to invoke specific services, clients simply talk to the gateway. The API Gateway provides each kind of client with a specific API. This reduces the number of round trips between the client and application. It also simplifies the client code.

Drawback: The API Gateway also has some drawbacks. It is yet another highly available component that must be developed, deployed, and managed. There is also a risk that the API Gateway becomes a development bottleneck. Developers must update the API Gateway in order to expose each microservice’s endpoints. It is important that the process for updating the API Gateway be as lightweight as possible. Otherwise, developers will be forced to wait in line in order to update the gateway.


#### Implementing an API Gateway

Various design issues you need to consider.

##### Performance and Scalability

Only a handful of companies operate at the scale of Netflix and need to handle billions of requests per day. However, for most applications the performance and scalability of the API Gateway is usually very important. It makes sense, therefore, to build the API Gateway on a platform that supports asynchronous, nonblocking I/O. There are a variety of different technologies that can be used to implement a scalable API Gateway. 

On the JVM you can use one of the NIO‑based frameworks such Netty, Vertx, Spring Reactor, or JBoss Undertow. One popular non‑JVM option is Node.js, which is a platform built on Chrome’s JavaScript engine. Another option is to use NGINX Plus. NGINX Plus offers a mature, scalable, high‑performance web server and reverse proxy that is easily deployed, configured, and programmed. NGINX Plus can manage authentication, access control, load balancing requests, caching responses, and provides application‑aware health checks and monitoring.

##### Using a Reactive Programming Model

The API Gateway handles some requests by simply routing them to the appropriate backend service. It handles other requests by invoking multiple backend services and aggregating the results. With some requests, such as a product details request, the requests to backend services are independent of one another. 

In order to minimize response time, the API Gateway should perform independent requests concurrently. Sometimes, however, there are dependencies between requests. The API Gateway might first need to validate the request by calling an authentication service, before routing the request to a backend service. Similarly, to fetch information about the products in a customer’s wish list, the API Gateway must first retrieve the customer’s profile containing that information, and then retrieve the information for each product.

Writing API composition code using the traditional asynchronous callback approach quickly leads you to callback hell. The code will be tangled, difficult to understand, and error‑prone. A much better approach is to write API Gateway code in a declarative style using a reactive approach. Examples of reactive abstractions include Future in Scala, CompletableFuture in Java 8, and Promise in JavaScript. There is also Reactive Extensions (also called Rx or ReactiveX), which was originally developed by Microsoft for the .NET platform. Netflix created RxJava for the JVM specifically to use in their API Gateway. There is also RxJS for JavaScript, which runs in both the browser and Node.js. Using a reactive approach will enable you to write simple yet efficient API Gateway code.

##### Service Invocation

A microservices‑based application is a distributed system and must use an inter‑process communication mechanism. There are two styles of inter‑process communication. One option is to use an asynchronous, messaging‑based mechanism. Some implementations use a message broker such as JMS or AMQP.

Others, such as Zeromq, are brokerless and the services communicate directly. The other style of inter‑process communication is a synchronous mechanism such as HTTP or Thrift. A system will typically use both asynchronous and synchronous styles. It might even use multiple implementations of each style. Consequently, the API Gateway will need to support a variety of communication mechanisms (Websocket etc). 

##### Service Discovery

The API Gateway needs to know the location (IP address and port) of each microservice with which it communicates. In a traditional application, you could probably hardwire the locations, but in a modern, cloud‑based microservices application this is a nontrivial problem. Infrastructure services, such as a message broker, will usually have a static location, which can be specified via OS environment variables. 

However, determining the location of an application service is not so easy. Application services have dynamically assigned locations. Also, the set of instances of a service changes dynamically because of autoscaling and upgrades. Consequently, the API Gateway, like any other service client in the system, needs to use the system’s service discovery mechanism: either Server‑Side Discovery or Client‑Side Discovery. 

##### Handling Partial Failures

Another issue you have to address when implementing an API Gateway is the problem of partial failure. This issue arises in all distributed systems whenever one service calls another service that is either responding slowly or is unavailable. 

The API Gateway should never block indefinitely waiting for a downstream service. However, how it handles the failure depends on the specific scenario and which service is failing. For example, if the recommendation service of the application is unresponsive in the product details scenario, the API Gateway should return the rest of the product details to the client since they are still useful to the user. The recommendations could either be empty or replaced by, for example, a hardwired top ten list. 

If, however, the product information service is unresponsive then API Gateway should return an error to the client.


The API Gateway could also return cached data if that was available. The data can be cached by the API Gateway itself or be stored in an external cache such as Redis or Memcached. By returning either default data or cached data, the API Gateway ensures that system failures do not impact the user experience.

A very good robust and scalable API Gateway must time out calls that exceed the specified threshold. It implements a circuit breaker pattern, which stops the client from waiting needlessly for an unresponsive service, can be added to the model's implementation. If the error rate for a service exceeds a specified threshold, it could trip a circuit breaker and all requests will fail immediately for a specified period of time. The module should define a fallback action when a request fails, such as reading from a cache or returning a default value. 

For most microservices‑based applications, it makes sense to implement an API Gateway, which acts as a single entry point into a system. The API Gateway is responsible for request routing, composition, and protocol translation. It provides each of the application’s clients with a custom API. The API Gateway can also mask failures in the backend services by returning cached or default data. 

# Containerisation

Source code of any  program cannot fully describe the function of that program without the context it will be compiled and run in. Most unexpected behaviour during deployment comes from build environments being different than expected. To make deployment repeatable, we need to make a program’s context repeatable. That’s where Docker comes in.

Docker essentially allows you to specify a “source code” for a program’s context that can then be “compiled” to an image and run as a container. This means that once we have tested an image we can have high confidence that it will perform equally well in every environment it is deployed to.

Additionally, Docker allows you to specify deploy configurations made up of multiple containers all linked in a private network and DNS that allows services that depend strongly on each other to be deployed and scaled together. 

To be fully context-agnostic, deployment should be able to happen to any host on the network on whatever port the host happens to have free. This presents a challenge: how do services link up when their network locations are fluid? You need a reverse proxy (like nginx) and a way to keep its configuration up to date in a changing service landscape.
You will need a  Consul to store and  manage service states.


#### What a deployment looks like:

	1.	Build Docker image.

	2.	Test that image in isolation.

	3.	Push that image to the in-house image registry.

	4.	Pull all images you need to deploy linked.

	5.	Deploy them to a test environment.

	6.	Run automated tests against the container system.

	7.	Upload service configuration to Consul API (if changed).

	8.	Deploy the containers to all hosts, tagged with the offline colour.

	9.	Wait until they are all responding and passing automated checks.

	10.	Flip environment alias to point at the offline colour.

	11.	The new build is now online.

#### To Do:: Docker configuration set up:: Code



#### Ultimate development/production workflow. How to get changes from your local machine into production:

	1.	Preferred development workflow involves the use of project management tracking tools like JIRA,
		Confluence. Developing practices like TDD, Unit Testing, XP, BDD. 
		The use of source control like GIT/SVN and continuous integration tools like Jenkins and Hudson

	2.   Typical development workflow will follow software development lifecyle:

		a. Requirement gathering

		b. Participation in system and interface design with Architects and UX Team

		c. Lead SPRINT Planning (Define SPRINT goals)

		d. Refining backlog with Product Owners and Team

		e. Create tickets and associate estimations and  resources with team

		f. Leading code reviews

		g. Ensuring Testing Framework is in place to check deliverables and requirement are met.

		h. Owning the code base and release cycles

		i. Working with Platform team to mange deployment and release management

		j. Managing bug fixes, and making sure regression tests are fully fulfilled

		k. Supporting aftercare (Hypercare)

		l. Refining and improving product and platform


	3	How to ensure that the quality of your code is high, and meets the standards of the project:

		1. Having best practices in place and supporting continuos delivery. 

		2. Using Automated Testing

		3. Having  a code review process in place. 

		4. Reducing manual testing times and spending more time on improving code quality

		5. Follow and support development and usability standards proclaimed by W3C

		6. Breaking features into smaller modules and having  test framework and documentation in 
		   place to support.

		7. Ensuring incremental releases. As this will allow to mange volume of flaws

		8. Have a micro-service architecture in place that supports loose coupling and  
		   ensures features are shipped with their context and configuration 
		   management services. 

		9. No configurations will sit outside the features that are developed. 
		   They will be contained in the same repository. This will be to avoid platform configuration 
		   changes that can potentially result in code and applications breaking. 
		   Use Docker, Puppet and Vagrant to support the application build

		10. Have documentation and dashboard available and easily accessible to 
		    the development team and stakeholders

		11. Ensure there is consistent and constant communication in the team 
		    with actions and setup agenda.


	4.	Improving and managing  complicated API random calls to disparate systems:

		The API data flow to the the UI environment may initially consist of making several 
		API calls to a variable number of  different data sources from the backend. 
		May include  Machine Learning servers, ElasticSearch indexes, 
		H-BASE and CORE  source. Making these numbers of API calls to such disparate  
		systems is  expensive and increases latency and expected response times. 

		Such an architecture can be re-vamped by  bulding a gatewawy to allow all the data 
		systems to manage and push into a central gateway poll, using for example Apache's ActiveMQ 
		as middleware, from where the UI backend will make a single call to the gateway pipeline.
		The gateway serves to save data and replicate across multiple services with
		stored changes.  It also allowed multiple  end points to subscribe to the service 
		that it provides.


	5.   Ensuring that websites and apps built are performant for your target audience, 
	     and address performance issues:

		a. Have as part of the infrastructure devices that are commonly used by the 
		   customers to  access and use the product during  development. This will allow you to  
		   capture some bottlenecks that could be obvious and inherent during development

		b. Research into  customer needs. This is fed into requirement gathering, 
		   analysed and turned into specification.

		c. Have automated debugging tools available and incorporated in the code building phase. 
		   Use Dev and debugging Tools built into browser. 
		   Use integrated test frameworks like BrowserStack, Selenium and  
		   BDD (Behavior Driven Design) to handle user stories and acceptance criteria.

		d. Have a usability testing phases whiles the product is being built. Done to 
		   gather customer feedback

		e. Ensure a beta  release of the application will be in place and create a segmentation 
		   for a cross section of customers to evaluate.

		   Normally segmentation is done with loyalty uses. These are reliable customers 
		   who use the application very often,so it is important to seek their opinion

		f. Engage positive feedback, workshops with customer services and platform/release 
		   management/DevOps to review customer concerns with view to refining product features.

		g. Use Analytics tools to build statistics and pattern of customer behaviour 
		   i.e - Omniture/Google Analytics



# Web Services

A web service is a collection of open protocols and standards  used for exchanging data between applications or systems over the internet and uses a standardized XML messaging  system. XML is used to encode all communications to a web service. For  example, a client invokes a web service by sending an XML message, then  waits for a corresponding XML response.

 As all communication is in XML,  web services are not tied to any one operating system or programming  language - Java can talk with Perl; Windows applications can talk with  Unix applications.

 Web services are self-contained, modular, distributed, dynamic  applications. These  applications can be local, distributed, or web-based. Web services are  built on top of open standards such as TCP/IP, HTTP, Java, HTML, and  XML.


# Differences between SOA and Micro-services

The core difference between SOA and microservices lies in the size and scope. Microservices must be independently deployable, whereas SOA services are often implemented in deployment monoliths. Classic SOA is more platform driven, so microservices offer more choices in all dimensions.

 Features of SOA:

	1. Boundaries are explicit

	2. Services are autonomous

	3. Services share schema and contract, not class

	4. Service compatibility is based on policy



